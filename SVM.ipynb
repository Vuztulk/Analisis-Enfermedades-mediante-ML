{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementación y Evaluación de un Modelo con SVM**\n",
    "\n",
    "En este proyecto, implementamos y evaluamos un modelo SVM para clasificar muestras de sangre en función de si presentan enfermedades o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importación de Librerías**\n",
    "\n",
    "Primero, importamos todas las bibliotecas necesarias para nuestro análisis. Esto incluye pandas y numpy para la manipulación de datos, sklearn para el aprendizaje automático y matplotlib para la visualización de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from scipy.stats import uniform, loguniform\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carga de Datos**\n",
    "\n",
    "Cargamos nuestros datos de un archivo CSV llamado ‘blood_samples.csv’. Separamos nuestras características (X) de nuestras etiquetas (y). Luego, dividimos nuestros datos en conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('blood_samples.csv')\n",
    "X = df_train.drop('Disease', axis=1)  # Caracteristicas\n",
    "y = df_train['Disease']  # Etiquetas\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación del Modelo SVM**\n",
    "\n",
    "Definimos una función para evaluar nuestro modelo SVM. Esta función entrena el modelo, realiza predicciones en el conjunto de prueba y luego imprime un informe de clasificación. También realiza una validación cruzada si se especifica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_svm(x_train, y_train, x_test, y_test, kernel='poly', do_cross_validation=True):\n",
    "\n",
    "    model = svm.SVC(kernel=kernel)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    predictions = model.predict(x_test)\n",
    "\n",
    "    print(\"Reporte de clasificación:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "    if do_cross_validation:\n",
    "        cv = cross_val_score(model, x_train, y_train, cv=5, scoring='accuracy')  # Número de iteraciones = 5\n",
    "\n",
    "        print(\"Resultados de validación cruzada:\")\n",
    "        for i, accuracy in enumerate(cv):\n",
    "            print(f\"Iteración {i+1}: Accuracy = {accuracy}\")\n",
    "\n",
    "        print(f\"Accuracy promedio: {np.mean(cv)}\")\n",
    "        print(f\"Desviación estándar del accuracy: {np.std(cv)}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, predictions, labels=model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    classes = list(report.keys())[:-3]\n",
    "    metrics = ['precision', 'recall', 'f1-score']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.2\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [report[key][metric] for key in classes]\n",
    "        ax.bar(x + width * i, values, width, label=metric)\n",
    "\n",
    "    ax.set_ylabel('Metricas')\n",
    "    ax.set_title('Metricas de clasificacion')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba de Diferentes Kernels**\n",
    "\n",
    "Probamos diferentes kernels para ver cuál proporciona los mejores resultados. Probamos los kernels ‘linear’, ‘poly’ y ‘rbf’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_svm(x_train, y_train, x_test, y_test, kernel='linear', do_cross_validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_svm(x_train, y_train, x_test, y_test, kernel='poly', do_cross_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_svm(x_train, y_train, x_test, y_test, kernel='rbf', do_cross_validation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimización de Hiperparámetros**\n",
    "\n",
    "Utilizamos RandomizedSearchCV y GridSearchCV para optimizar los hiperparámetros de nuestro modelo SVM. Estos métodos prueban diferentes combinaciones de hiperparámetros y seleccionan la combinación que produce la mayor precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El código optimiza, entrena y evalúa un modelo de Máquinas de Vectores de Soporte (SVM) con visualización comparativa de la precisión para diferentes parámetros. Se realizan los siguientes pasos:\n",
    "\n",
    "**Ajuste de hiperparámetros (Randomized Search CV):**\n",
    "\n",
    "Se define un espacio de búsqueda para los hiperparámetros del SVM (kernel, C, gamma, etc.).\n",
    "El modelo SVM se entrena con diferentes combinaciones de hiperparámetros utilizando validación cruzada y se evalúa su precisión.\n",
    "Se encuentra la mejor configuración de hiperparámetros según la mayor precisión media.\n",
    "\n",
    "**Entrenamiento y evaluación:**\n",
    "\n",
    "Se crea un modelo SVM final con los mejores hiperparámetros.\n",
    "El modelo final se entrena en los datos de entrenamiento.\n",
    "Se realizan predicciones en los datos de prueba.\n",
    "Se evalúa el rendimiento del modelo mediante un informe de clasificación y una matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    param_distributions = {\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'C': loguniform(0.001, 100.0),\n",
    "        'gamma': loguniform(0.00001, 1.0),\n",
    "        'degree': [2, 3, 4, 5],\n",
    "        'coef0': uniform(-1.0, 1.0),\n",
    "        'shrinking': [True, False],\n",
    "        'probability': [True, False],\n",
    "        'tol': loguniform(1e-6, 1e-2),\n",
    "        'cache_size': [100, 200, 500],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'verbose': [0],\n",
    "        'max_iter': [1000, 2000, 5000],\n",
    "        'decision_function_shape': ['ovo', 'ovr'],\n",
    "    }\n",
    "\n",
    "    model = svm.SVC()\n",
    "\n",
    "    random_search = RandomizedSearchCV(model, param_distributions, n_iter=100, cv=5, scoring='accuracy', n_jobs=8)\n",
    "\n",
    "    random_search.fit(x_train, y_train)\n",
    "\n",
    "    best_params = random_search.best_params_\n",
    "\n",
    "    print(\"Mejores parámetros:\", best_params)\n",
    "    print(\"Mejor puntaje:\", random_search.best_score_)\n",
    "\n",
    "    best_model = svm.SVC(**best_params)\n",
    "    best_model.fit(x_train, y_train)\n",
    "\n",
    "    predictions = best_model.predict(x_test)\n",
    "    print(\"Reporte de clasificación:\\n\", classification_report(y_test, predictions))\n",
    "    cm = confusion_matrix(y_test, predictions, labels = best_model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = best_model.classes_)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploteamos los distintos parametros en relaccion a su accuracy, pero como RandomizedSearchCV mezcla los parametros de manera aleatoria, los graficos no tendran una relaccion consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "params_of_interest = ['param_kernel', 'param_C', 'param_gamma', 'param_degree']\n",
    "\n",
    "for param in params_of_interest:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Filtrar el DataFrame para eliminar las filas donde el parámetro es None\n",
    "    filtered_df = df_random[df_random[param].notna()]\n",
    "    plt.plot(filtered_df[param], filtered_df['mean_test_score'], 'o')\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel('Mean Test Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvermos a hacer la busqueda pero con GridSearchCV para que la busqueda sea mas exhaustiva y consistente en los parametros que con RandomizedSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    param_grid = {\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'C': np.logspace(-3, 2, 6),\n",
    "        'gamma': np.logspace(-5, 0, 6),\n",
    "        'degree': [2, 3],\n",
    "        'shrinking': [True],\n",
    "        'probability': [True],\n",
    "        'class_weight': [None],\n",
    "        'verbose': [0],\n",
    "        'max_iter': [1000],\n",
    "        'decision_function_shape': ['ovr'],\n",
    "    }\n",
    "\n",
    "    model = svm.SVC()\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=8)\n",
    "\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(\"Mejores parámetros:\", best_params)\n",
    "    print(\"Mejor puntaje:\", grid_search.best_score_)\n",
    "\n",
    "    best_model = svm.SVC(**best_params)\n",
    "    best_model.fit(x_train, y_train)\n",
    "\n",
    "    predictions = best_model.predict(x_test)\n",
    "    print(\"Reporte de clasificación:\\n\", classification_report(y_test, predictions))\n",
    "    cm = confusion_matrix(y_test, predictions, labels = best_model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = best_model.classes_)\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "params_of_interest = ['param_kernel', 'param_C', 'param_gamma', 'param_degree']\n",
    "\n",
    "for param in params_of_interest:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    filtered_df = df_grid[df_grid[param].notna()]\n",
    "    plt.plot(filtered_df[param], filtered_df['mean_test_score'], 'o')\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel('Mean Test Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados con RandomizedSearchCV\n",
    "\n",
    "    Mejores parámetros: {\n",
    "        'C': 96.39636288596346,\n",
    "        'cache_size': 200,\n",
    "        'class_weight': 'balanced',\n",
    "        'coef0': -0.6719290189526498,\n",
    "        'decision_function_shape': 'ovo',\n",
    "        'degree': 4,\n",
    "        'gamma': 0.32329765890129275,\n",
    "        'kernel': 'rbf',\n",
    "        'max_iter': 2000,\n",
    "        'probability': True,\n",
    "        'shrinking': True,\n",
    "        'tol': 0.00025189034577900667,\n",
    "        'verbose': 0\n",
    "        }\n",
    "\n",
    "    Mejor puntaje: 0.9435909404751486\n",
    "    \n",
    "Resultados con GridSearchCV\n",
    "\n",
    "    Mejores parámetros: {\n",
    "        'C': 10.0,\n",
    "        'class_weight': None,\n",
    "        'decision_function_shape': 'ovr',\n",
    "        'degree': 2,\n",
    "        'gamma': 1.0,\n",
    "        'kernel': 'rbf',\n",
    "        'max_iter': 1000,\n",
    "        'probability': True,\n",
    "        'shrinking': True,\n",
    "        'verbose': 0\n",
    "        }\n",
    "        \n",
    "    Mejor puntaje: 0.9418268809989205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validación Cruzada Estratificada**\n",
    "\n",
    "Realizamos una validación cruzada estratificada para evaluar la robustez de nuestro modelo. Esto implica dividir nuestros datos en diferentes particiones de manera que cada partición tenga aproximadamente la misma proporción de clases que los datos originales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `stratified_cross_validation` realiza una validación cruzada estratificada en un conjunto de datos utilizando un modelo SVC, y como previamente hemos sacado los hiperparametros optimos usare esos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_cross_validation(X, y, model, n_splits=5):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    print(f\"Precision por particion: {accuracies}\")\n",
    "    print(f\"Precision media: {np.mean(accuracies)}\")\n",
    "    print(f\"Desviacion estandar de la precision: {np.std(accuracies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=10.0,class_weight=None,decision_function_shape='ovr',degree=2,gamma=1.0,kernel='rbf',max_iter=1000,probability=True,shrinking=True)\n",
    "accuracies = stratified_cross_validation(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precisión por partición**: Rendimiento bueno en las 3 primeras particiones pero disminuye en los 2 últimos. Esto podría indicar que mi modelo está sobreajustando ciertas partes de los datos y no generaliza bien a otras partes.\n",
    "\n",
    "**Precisión media**: El modelo fue correcto un 92% de las veces en las particiones.\n",
    "\n",
    "**Desviación estándar de la precisión**: La desviación estándar es de 0.10, lo que indica que hubo una variabilidad del 10% en las precisiones de las particiones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curva ROC**\n",
    "\n",
    "Finalmente, trazamos la Curva de Característica Operativa del Receptor (ROC) para evaluar el rendimiento de nuestro modelo. La curva ROC es una gráfica que muestra la tasa de verdaderos positivos en función de la tasa de falsos positivos para diferentes umbrales de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SVC(C=10.0,class_weight=None,decision_function_shape='ovr',degree=2,gamma=1.0,kernel='rbf',max_iter=1000,probability=True,shrinking=True)\n",
    "y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "best_model.fit(x_train, y_train)\n",
    "y_score = best_model.predict_proba(x_test)\n",
    "\n",
    "fpr = []\n",
    "tpr = []\n",
    "roc_auc = []\n",
    "lw=2\n",
    "\n",
    "for i, class_name in enumerate(np.unique(y_test)):\n",
    "    fpr_i, tpr_i, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc_i = auc(fpr_i, tpr_i)\n",
    "    fpr.append(fpr_i)\n",
    "    tpr.append(tpr_i)\n",
    "    roc_auc.append(roc_auc_i)\n",
    "\n",
    "    plt.plot(fpr_i, tpr_i, lw=lw, label='{0} = {1:0.2f}'.format(class_name, roc_auc_i))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Ratio de falso positivo')\n",
    "plt.ylabel('Ratio de verdadero positivo')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estos valores indican que el modelo tiene un excelente rendimiento en la clasificación de las clases**, ya que los valores AUC están muy cerca de 1, que es el valor óptimo. Un valor AUC de 1 significa que el modelo tiene una capacidad muy buena para distinguir entre la clase en cuestión y las demás clases. Un valor AUC de 0.5, representado por la línea diagonal punteada, indicaría que el modelo no tiene ninguna capacidad de clasificación más allá de la aleatoriedad. **Por lo tanto, los valores AUC cercanos a 1 son muy buenos**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruebas de robustez**\n",
    "\n",
    "Vamos a realizar pruebas de robustez para ver como se comporta el modelo ante cambios pequeños en los datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sizes = np.arange(0.1, 1.0, 0.1)\n",
    "accuracies = []\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "plt.plot(test_sizes, accuracies, marker='o')\n",
    "plt.title('Precisión del modelo en funcion del tamaño del conjunto de prueba')\n",
    "plt.xlabel('Tamaño del conjunto de prueba')\n",
    "plt.ylabel('Precisión')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Con un tamaño de prueba del 10%** (es decir, el 90% de los datos se utilizan para el entrenamiento), la precision del modelo es del 94.01%.\n",
    "\n",
    "**Con un tamaño de prueba del 20%,** la precision aumenta ligeramente a 94.72%.\n",
    "\n",
    "**Con un tamaño de prueba del 30%,** la precision disminuye un poco a 94.25%.\n",
    "\n",
    "**Con un tamaño de prueba del 40%,** la precision es del 94.63%.\n",
    "\n",
    "**Con un tamaño de prueba del 50%,** la precision disminuye a 93.80%.\n",
    "\n",
    "**Con un tamaño de prueba del 60%,** la precision disminuye aun mas a 93.66%.\n",
    "\n",
    "**Con un tamaño de prueba del 70%,** la precision aumenta ligeramente a 94.01%.\n",
    "\n",
    "**Con un tamaño de prueba del 80%,** la precision disminuye significativamente a 92.47%.\n",
    "\n",
    "**Con un tamaño de prueba del 90%,** la precision aumenta ligeramente a 92.60%.\n",
    "\n",
    "\n",
    "En general, estos resultados sugieren que el modelo tiene un rendimiento bastante robusto, ya que la precision se mantiene en torno al 94% para la mayoria de los tamaños de prueba. Sin embargo, hay una disminucion notable en la precision cuando el tamaño de la prueba aumenta al 80% y 90%. Esto podria indicar que el modelo se beneficia de tener mas datos para el entrenamiento y podria tener dificultades para generalizar bien cuando se entrena con menos datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
